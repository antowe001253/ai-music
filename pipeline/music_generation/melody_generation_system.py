#!/usr/bin/env python3
"""
FIXED Melody Generation System for Automated Music Pipeline
Uses stable model to avoid the issues we discovered
"""

import torch
from transformers import MusicgenForConditionalGeneration, AutoProcessor
import librosa
import numpy as np
import scipy.io.wavfile
from pathlib import Path
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from audio_processing import AudioProcessingSuite
except ImportError:
    print("Warning: AudioProcessingSuite not found, some features may be limited")
    AudioProcessingSuite = None

class MelodyGenerationSystem:
    def __init__(self, device="auto"):
        """
        Initialize melody generation system with FIXED model selection
        
        Args:
            device: Device to use ('auto', 'cpu', 'cuda', 'mps')
        """
        self.device = self._get_device(device)
        self.melody_model = None
        self.melody_processor = None
        
        if AudioProcessingSuite:
            self.audio_processor = AudioProcessingSuite()
        else:
            self.audio_processor = None
        
    def _get_device(self, device):
        """Determine the best device to use"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device
    
    def load_melody_model(self):
        """Load MusicGen model - FIXED to use stable version"""
        print(f"Loading STABLE MusicGen model on {self.device}...")
        
        # Try models in order of stability
        models_to_try = [
            "facebook/musicgen-small",     # Back to what was loading
        ]
        
        for model_name in models_to_try:
            try:
                print(f"ğŸ”„ Trying {model_name}...")
                
                self.melody_processor = AutoProcessor.from_pretrained(model_name)
                self.melody_model = MusicgenForConditionalGeneration.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,  # Stable dtype
                    low_cpu_mem_usage=True      # Memory optimization
                )
                
                # Check for vocab mismatch before moving to device
                if hasattr(self.melody_processor.tokenizer, 'vocab_size') and hasattr(self.melody_model.config, 'text_encoder'):
                    tokenizer_vocab = self.melody_processor.tokenizer.vocab_size
                    model_vocab = self.melody_model.config.text_encoder.vocab_size
                    
                    if tokenizer_vocab != model_vocab:
                        print(f"âš ï¸ Vocab mismatch detected in {model_name}: {tokenizer_vocab} vs {model_vocab}")
                        if model_name != "facebook/musicgen-small":
                            print("ğŸ”„ Trying next model...")
                            continue
                        else:
                            print("ğŸ”§ Applying minimal vocab fix...")
                            # Apply minimal vocab fix - only add what's needed
                            tokens_needed = model_vocab - tokenizer_vocab
                            if tokens_needed > 0 and tokens_needed < 100:  # Safety limit
                                for i in range(tokens_needed):
                                    self.melody_processor.tokenizer.add_tokens([f'<pad_{i}>'])
                                print(f"âœ… Added {tokens_needed} tokens to match model vocab")
                
                # Move to device if not CPU
                if self.device != "cpu":
                    self.melody_model = self.melody_model.to(self.device)
                
                print(f"âœ… {model_name} loaded successfully on {self.device}")
                self.current_model = model_name
                return True
                
            except Exception as e:
                print(f"âŒ Error loading {model_name}: {e}")
                if model_name == models_to_try[-1]:
                    print("âŒ All models failed to load")
                    return False
                continue
        
        return False
    
    def generate_melody_conditioned(self, text_description, melody_audio=None, duration=10):
        """
        Generate melody-conditioned music with FIXED parameters
        
        Args:
            text_description: Text description of the desired music
            melody_audio: Optional melody audio to condition on
            duration: Duration in seconds
        """
        if self.melody_model is None:
            print("âŒ Melody model not loaded. Call load_melody_model() first.")
            return None
        
        try:
            print(f"ğŸµ Generating music: '{text_description}'")
            
            # Simplified input processing to avoid tokenization issues
            safe_description = text_description.lower().replace("song", "music")
            if len(safe_description.split()) > 5:
                safe_description = " ".join(safe_description.split()[:5])
            
            # Process input
            inputs = self.melody_processor(
                text=[safe_description],
                padding=True,
                return_tensors="pt",
                truncation=True,
                max_length=128
            )
            
            # Move inputs to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Conservative generation parameters
            sample_rate = self.melody_model.config.audio_encoder.sampling_rate
            max_new_tokens = min(1500, max(256, int(duration * 50)))  # Back to session_508d302d settings
            
            print(f"ğŸ“Š Generating {max_new_tokens} tokens at {sample_rate}Hz")
            
            with torch.no_grad():
                audio_values = self.melody_model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=1.0,
                    top_k=250,
                    top_p=0.0,
                    num_beams=1,
                    pad_token_id=self.melody_processor.tokenizer.pad_token_id or self.melody_processor.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            # Process generated audio
            audio_np = audio_values[0, 0].cpu().numpy()
            
            # Normalize
            if np.max(np.abs(audio_np)) > 0:
                audio_np = audio_np / np.max(np.abs(audio_np)) * 0.9
            
            print(f"âœ… Generated {len(audio_np)/sample_rate:.1f}s of audio")
            
            return {
                'audio': audio_np,
                'sample_rate': sample_rate,
                'description': safe_description,
                'model': self.current_model,
                'duration': len(audio_np) / sample_rate
            }
            
        except Exception as e:
            print(f"âŒ Generation failed: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def save_generated_audio(self, audio_data, output_path, sample_rate=None):
        """Save generated audio to file"""
        if audio_data is None:
            return False
        
        try:
            if isinstance(audio_data, dict):
                audio_np = audio_data['audio']
                sample_rate = audio_data['sample_rate']
            else:
                audio_np = audio_data
                sample_rate = sample_rate or 32000
            
            # Convert to int16
            audio_int16 = (audio_np * 32767).astype(np.int16)
            
            # Save
            scipy.io.wavfile.write(str(output_path), sample_rate, audio_int16)
            print(f"ğŸ’¾ Audio saved to: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Save failed: {e}")
            return False

# Test function
if __name__ == "__main__":
    print("ğŸ§ª Testing Fixed Melody Generation System...")
    
    system = MelodyGenerationSystem()
    
    if system.load_melody_model():
        print("âœ… Model loaded successfully!")
        
        # Test generation
        result = system.generate_melody_conditioned("Christmas carol music", duration=15)
        
        if result:
            print("âœ… Generation successful!")
            
            # Save test output
            output_path = Path("test_fixed_melody.wav")
            if system.save_generated_audio(result, output_path):
                print(f"ğŸµ Test audio saved to: {output_path}")
        else:
            print("âŒ Generation failed")
    else:
        print("âŒ Model loading failed")
